{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpikeInterface pipeline for Mease Lab - CED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spikeextractors as se\n",
    "import spiketoolkit as st\n",
    "import spikesorters as ss\n",
    "import spikecomparison as sc\n",
    "import spikewidgets as sw\n",
    "\n",
    "from mease_lab_to_nwb.convert_ced.cednwbconverter import quick_write\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load CED recording, set channel locations, compute LFP, and inspect signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ced_file = Path(\n",
    "    \"/home/luiz/storage/taufferconsulting/client_ben/project_heidelberg_gui/heidelberg_data/CED_example_data/M365/pt1 15 + mech.smrx\"\n",
    ")\n",
    "# ced_file = Path('/Users/abuccino/Documents/Data/catalyst/heidelberg/ced/m365_pt1_590-1190secs-001.smrx')\n",
    "# ced_file = Path('D:/CED_example_data/Other example/m365_pt1_590-1190secs-001.smrx')\n",
    "probe_file = \"../probe_files/cambridge_neurotech_H3.prb\"\n",
    "spikeinterface_folder = ced_file.parent\n",
    "spikeinterface_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically select Rhd channels\n",
    "channel_info = se.CEDRecordingExtractor.get_all_channels_info(ced_file)\n",
    "\n",
    "rhd_channels = []\n",
    "for ch, info in channel_info.items():\n",
    "    if \"Rhd\" in info[\"title\"]:\n",
    "        rhd_channels.append(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = se.CEDRecordingExtractor(ced_file, smrx_channel_ids=rhd_channels)\n",
    "\n",
    "# Load probe file to re-order channels and add location\n",
    "recording = se.load_probe_file(recording, probe_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw.plot_electrode_geometry(recording)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate multiple recordings \n",
    "\n",
    "With the `MultiRecordingTimeExtractor`, you can easily concatenate the multiple recordings in time. The recordings must have the same channels and locations (e.g. same probe file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we concatenate the same file as an example\n",
    "recording_files = [ced_file, ced_file, ced_file]\n",
    "\n",
    "recordings = []\n",
    "for file in recording_files:\n",
    "    # Automatically select Rhd channels\n",
    "    channel_info = se.CEDRecordingExtractor.get_all_channels_info(file)\n",
    "\n",
    "    rhd_channels = []\n",
    "    for ch, info in channel_info.items():\n",
    "        if \"Rhd\" in info[\"title\"]:\n",
    "            rhd_channels.append(ch)\n",
    "    recording = se.CEDRecordingExtractor(ced_file, smrx_channel_ids=rhd_channels)\n",
    "    recording = se.load_probe_file(recording, probe_file)\n",
    "    recordings.append(recording)\n",
    "# instantiate a MultiRecording object\n",
    "multirecording = se.MultiRecordingTimeExtractor(recordings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `multirecording` is also a `RecordingExtractor` and it can be used for further processing. It contains `epoch` information about start and end of each recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_name in multirecording.get_epoch_names():\n",
    "    print(multirecording.get_epoch_info(epoch_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have separate files from different probes recorded using the same device, you can concatenate them in the channel dimension. You can add separate groups to the different recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we concatenate the same recording as an example\n",
    "multirec_group = se.MultiRecordingChannelExtractor(\n",
    "    [recording, recording], groups=[0, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multirec_group.get_channel_groups())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different groups can be spike sorted separately using the `grouping_property='group'` argument. The when spike sorting by group, the output units have a property called `group` with info about which group it's been found on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that MultiRecordingTimeExtractor and ChannelExtractor can be easily combined!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num channels: {recording.get_num_channels()}\")\n",
    "print(f\"Sampling rate: {recording.get_sampling_frequency()}\")\n",
    "print(\n",
    "    f\"Duration (s): {recording.get_num_frames() / recording.get_sampling_frequency()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_channels = []\n",
    "for ch, info in channel_info.items():\n",
    "    if \"LFP\" in info[\"title\"]:\n",
    "        lfp_channels.append(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_lfp = se.CEDRecordingExtractor(ced_file, smrx_channel_ids=lfp_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sampling frequency AP: {recording.get_sampling_frequency()}\")\n",
    "print(f\"Sampling frequency LF: {recording_lfp.get_sampling_frequency()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Resample LFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_lfp = st.preprocessing.resample(recording_lfp, resample_rate=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ts_ap = sw.plot_timeseries(recording, trange=[10, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ts_lf = sw.plot_timeseries(recording_lfp, trange=[30, 40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_filter = False  # the CED data appear to be already filtered\n",
    "apply_cmr = True\n",
    "freq_min_hp = 300\n",
    "freq_max_hp = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?st.preprocessing.common_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_filter:\n",
    "    recording_processed = st.preprocessing.bandpass_filter(\n",
    "        recording, freq_min=freq_min_hp, freq_max=freq_max_hp\n",
    "    )\n",
    "else:\n",
    "    recording_processed = recording\n",
    "\n",
    "if apply_cmr:\n",
    "    recording_processed = st.preprocessing.common_reference(recording_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stub recording for fast testing; set to False for running processing pipeline on entire data\n",
    "stub_test = True\n",
    "nsec_stub = 30\n",
    "\n",
    "if stub_test:\n",
    "    recording_processed = se.SubRecordingExtractor(\n",
    "        parent_recording=recording_processed,\n",
    "        end_frame=int(nsec_stub * recording_processed.get_sampling_frequency()),\n",
    "    )\n",
    "    recording_lfp = se.SubRecordingExtractor(\n",
    "        recording_lfp, end_frame=int(nsec_stub * recording_lfp.get_sampling_frequency())\n",
    "    )\n",
    "\n",
    "print(f\"Original signal length: {recording.get_num_frames()}\")\n",
    "print(f\"Processed signal length: {recording_processed.get_num_frames()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = recording_processed.get_num_frames()\n",
    "print(num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ts_ap = sw.plot_timeseries(recording_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Run spike sorters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.installed_sorters()\n",
    "# ss.IronClustSorter.set_ironclust_path(\"D:/GitHub/ironclust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorter_list = [\n",
    "    \"herdingspikes\",\n",
    "    #     \"ironclust\",\n",
    "    \"klusta\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect sorter-specific parameters and defaults\n",
    "for sorter in sorter_list:\n",
    "    print(f\"\\n\\n{sorter} params description:\")\n",
    "    pprint(ss.get_params_description(sorter))\n",
    "    print(\"Default params:\")\n",
    "    pprint(ss.get_default_params(sorter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-specific parameters\n",
    "sorter_params = dict(\n",
    "    #     ironclust={'detect_threshold': 6},\n",
    "    klusta={},\n",
    "    herdingspikes={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?ss.run_sorters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_outputs = ss.run_sorters(\n",
    "    sorter_list=sorter_list,\n",
    "    working_folder=spikeinterface_folder / \"ced_si_output\",\n",
    "    recording_dict_or_list=dict(rec0=recording_processed),\n",
    "    sorter_params=sorter_params,\n",
    "    mode=\"overwrite\",  # change to \"keep\" to avoid repeating the spike sorting\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sorting_outputs` is a dictionary with (\"rec_name\", \"sorter_name\") as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result_name, sorting in sorting_outputs.items():\n",
    "    rec_name, sorter = result_name\n",
    "    print(f\"{sorter} found {len(sorting.get_unit_ids())} units\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split sorting output when concatenation is used\n",
    "\n",
    "If you concatenated multiple recordings into a `MultiRecordingTimeExtractor`, you can split the sorting output using the epoch information and the `SubSortingExtractor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(recording_processed, se.MultiRecordingTimeExtractor):\n",
    "    sortings_split = []\n",
    "    sorting_to_be_split = sorting_outputs[(\"rec0\", \"ironclust\")]\n",
    "\n",
    "    for epoch_name in recording_processed.get_epoch_names():\n",
    "        epoch_info = multirecording.get_epoch_info(epoch_name)\n",
    "        sorting_split = se.SubSortingExtractor(\n",
    "            sorting_to_be_split,\n",
    "            start_frame=epoch_info[\"start_frame\"],\n",
    "            end_frame=epoch_info[\"end_frame\"],\n",
    "        )\n",
    "        sortings_split.append(sorting_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Post-processing: extract waveforms, templates, quality metrics, extracellular features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set postprocessing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing params\n",
    "postprocessing_params = st.postprocessing.get_common_params()\n",
    "postprocessing_params[\"verbose\"] = True\n",
    "postprocessing_params[\"recompute_info\"] = True\n",
    "pprint(postprocessing_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) change parameters\n",
    "postprocessing_params[\n",
    "    \"max_spikes_per_unit\"\n",
    "] = 1000  # with None, all waveforms are extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note for Windows**: on Windows, we currently have some problems with the `memmap` argument. While we fix it, we recommend to set it to `False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set quality metric list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality metrics\n",
    "qc_list = st.validation.get_quality_metrics_list()\n",
    "print(f\"Available quality metrics: {qc_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) define subset of qc\n",
    "qc_list = [\"snr\", \"isi_violation\", \"firing_rate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set extracellular features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracellular features\n",
    "ec_list = st.postprocessing.get_template_features_list()\n",
    "print(f\"Available EC features: {ec_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) define subset of ec\n",
    "ec_list = [\"peak_to_valley\", \"halfwidth\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocess all sorting outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result_name, sorting in sorting_outputs.items():\n",
    "    rec_name, sorter = result_name\n",
    "    print(f\"Postprocessing recording {rec_name} sorted with {sorter}\")\n",
    "    tmp_folder = Path(\"tmp_ced\") / sorter\n",
    "    tmp_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # set local tmp folder\n",
    "    sorting.set_tmp_folder(tmp_folder)\n",
    "\n",
    "    # compute waveforms\n",
    "    waveforms = st.postprocessing.get_unit_waveforms(\n",
    "        recording_processed, sorting, n_jobs=16, chunk_mb=2000, **postprocessing_params\n",
    "    )\n",
    "\n",
    "    # compute templates\n",
    "    templates = st.postprocessing.get_unit_templates(\n",
    "        recording_processed, sorting, **postprocessing_params\n",
    "    )\n",
    "\n",
    "    # comput EC features\n",
    "    ec = st.postprocessing.compute_unit_template_features(\n",
    "        recording_processed, sorting, feature_names=ec_list, as_dataframe=True\n",
    "    )\n",
    "    # compute QCs\n",
    "    qc = st.validation.compute_quality_metrics(\n",
    "        sorting, recording=recording_processed, metric_names=qc_list, as_dataframe=True\n",
    "    )\n",
    "\n",
    "    # export to phy example\n",
    "    if sorter == \"ironclust\":\n",
    "        phy_folder = spikeinterface_folder / \"phy\" / sorter\n",
    "        phy_folder.mkdir(parents=True, exist_ok=True)\n",
    "        print(\"Exporting to phy\")\n",
    "        st.postprocessing.export_to_phy(\n",
    "            recording_processed, sorting, phy_folder, verbose=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_ironclust = sorting_outputs[(\"rec0\", \"ironclust\")]\n",
    "print(f\"Properties: {sorting_ironclust.get_shared_unit_property_names()}\")\n",
    "print(f\"Spikefeatures: {sorting_ironclust.get_shared_unit_spike_feature_names()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Phy-curated data back to SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!phy template-gui /Users/abuccino/Documents/Data/catalyst/heidelberg/ced/phy/ironclust/params.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phy_folder = \"/Users/abuccino/Documents/Data/catalyst/heidelberg/ced/phy/ironclust/\"\n",
    "sorting_curated = se.PhySortingExtractor(phy_folder)\n",
    "print(f\"Units after manual curation: {len(sorting_curated.get_unit_ids())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Ensemble spike sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sorting_outputs) > 1:\n",
    "    # retrieve sortings and sorter names\n",
    "    sorting_list = []\n",
    "    sorter_names_comp = []\n",
    "    for result_name, sorting in sorting_outputs.items():\n",
    "        rec_name, sorter = result_name\n",
    "        sorting_list.append(sorting)\n",
    "        sorter_names_comp.append(sorter)\n",
    "\n",
    "    # run multisorting comparison\n",
    "    mcmp = sc.compare_multiple_sorters(\n",
    "        sorting_list=sorting_list, name_list=sorter_names_comp\n",
    "    )\n",
    "\n",
    "    # plot agreement results\n",
    "    w_agr = sw.plot_multicomp_agreement(mcmp)\n",
    "\n",
    "    # extract ensamble sorting\n",
    "    sorting_ensemble = mcmp.get_agreement_sorting(minimum_agreement_count=2)\n",
    "\n",
    "    print(\n",
    "        f\"Ensemble sorting among {sorter_list} found: {len(sorting_ensemble.get_unit_ids())} units\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Automatic curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define curators and thresholds\n",
    "isi_violation_threshold = 0.5\n",
    "snr_threshold = 3\n",
    "firing_rate_threshold = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortings = []\n",
    "sortings_auto_curated = []\n",
    "sorter_names_curation = []\n",
    "for result_name, sorting in sorting_outputs.items():\n",
    "    rec_name, sorter = result_name\n",
    "    sorter_names_curation.append(sorter)\n",
    "    sortings.append(sorting)\n",
    "\n",
    "    # firing rate threshold\n",
    "    sorting_curated = st.curation.threshold_firing_rates(\n",
    "        sorting,\n",
    "        duration_in_frames=num_frames,\n",
    "        threshold=firing_rate_threshold,\n",
    "        threshold_sign=\"less\",\n",
    "    )\n",
    "\n",
    "    # isi violation threshold\n",
    "    sorting_curated = st.curation.threshold_isi_violations(\n",
    "        sorting_curated,\n",
    "        duration_in_frames=num_frames,\n",
    "        threshold=isi_violation_threshold,\n",
    "        threshold_sign=\"greater\",\n",
    "    )\n",
    "\n",
    "    # isi violation threshold\n",
    "    sorting_curated = st.curation.threshold_snrs(\n",
    "        sorting_curated,\n",
    "        recording=recording_processed,\n",
    "        threshold=snr_threshold,\n",
    "        threshold_sign=\"less\",\n",
    "    )\n",
    "    sortings_auto_curated.append(sorting_curated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (sort_name, sort, sort_curated) in zip(\n",
    "    sorter_names_curation, sortings, sortings_auto_curated\n",
    "):\n",
    "    print(f\"{sort_name}\")\n",
    "    print(f\"Units before curation: {len(sort.get_unit_ids())}\")\n",
    "    print(f\"Units after curation: {len(sort_curated.get_unit_ids())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Show how to split sorting outputs!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Quick save to NWB; writes only the spikes and lfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To complete the full conversion for other types of data, either\n",
    "###    1) Run the external conversion script before this notebook, and append to it by setting overwrite=False below\n",
    "###    2) Run the external conversion script after this notebook, which will append the NWBFile you make here so long as overwrite=False in the external script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name your NWBFile and decide where you want it saved\n",
    "nwbfile_path = ced_file.parent / \"CED_test.nwb\"\n",
    "\n",
    "# Enter Session and Subject information here\n",
    "session_description = \"Enter session description here.\"\n",
    "\n",
    "# Manually insert the session start time\n",
    "session_start = datetime(1971, 1, 1)  # (Year, Month, Day)\n",
    "\n",
    "# Choose the sorting extractor from the notebook environment you would like to write to NWB\n",
    "# chosen_sorting_extractor = sorting_outputs[('rec0', 'ironclust')]\n",
    "# chosen_sorting_extractor = sorting_ensemble\n",
    "\n",
    "quick_write(\n",
    "    ced_file_path=ced_file,\n",
    "    session_description=session_description,\n",
    "    session_start=session_start,\n",
    "    save_path=nwbfile_path,\n",
    "    #     sorting=chosen_sorting_extractor,\n",
    "    recording_lfp=recording_lfp,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NWB file with widgets\n",
    "from pynwb import NWBFile, NWBHDF5IO\n",
    "from nwbwidgets import nwb2widget\n",
    "\n",
    "io = NWBHDF5IO(str(nwbfile_path), \"r\")\n",
    "nwbfile = io.read()\n",
    "nwb2widget(nwbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwbfile.processing[\"ecephys\"].data_interfaces[\"LFP\"].electrical_series[\n",
    "    \"ElectricalSeries\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
