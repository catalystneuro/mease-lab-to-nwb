{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpikeInterface pipeline for Mease Lab - Syntalos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from datetime import datetime, timedelta\n",
    "from isodate import duration_isoformat\n",
    "\n",
    "import spikeextractors as se\n",
    "import spiketoolkit as st\n",
    "import spikesorters as ss\n",
    "import spikecomparison as sc\n",
    "import spikewidgets as sw\n",
    "from nwb_conversion_tools.json_schema_utils import dict_deep_update\n",
    "from nwb_conversion_tools.conversion_tools import save_si_object\n",
    "\n",
    "from mease_lab_to_nwb import SyntalosNWBConverter, SyntalosRecordingExtractor, SyntalosRecordingInterface\n",
    "from mease_lab_to_nwb.convert_syntalos.syntalosnwbconverter import quick_write\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load Intan recordings, compute LFP, and inspect signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#syntalos_folder = Path('D:/Syntalos/Latest Syntalos Recording _20200730/intan-signals')\n",
    "syntalos_folder = Path('/Users/abuccino/Documents/Data/catalyst/heidelberg/syntalos/Latest_Syntalos_Recording_20200730/intan-signals/')\n",
    "spikeinterface_folder = syntalos_folder / \"spikeinterface\"\n",
    "probe_file = '../probe_files/tetrode_32.prb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = SyntalosRecordingExtractor(syntalos_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load probe file for tetrodes\n",
    "recording = se.load_probe_file(recording, probe_file)\n",
    "print(recording.get_channel_groups())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the probe file loads a dummy geometry to identify the different tetrodes\n",
    "sw.plot_electrode_geometry(recording)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stub recording for fast testing; set to False for running processing pipeline on entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stub_test = True\n",
    "nsec_stub = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute LFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_min_lfp = 1\n",
    "freq_max_lfp = 300\n",
    "freq_resample_lfp = 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_lfp = st.preprocessing.bandpass_filter(recording, freq_min=freq_min_lfp, freq_max=freq_max_lfp)\n",
    "recording_lfp = st.preprocessing.resample(recording_lfp, freq_resample_lfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sampling frequency AP: {recording.get_sampling_frequency()}\")\n",
    "print(f\"Sampling frequency LF: {recording_lfp.get_sampling_frequency()}\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ts_ap = sw.plot_timeseries(recording, color_groups=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ts_lf = sw.plot_timeseries(recording_lfp, trange=[10, 15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_filter = True\n",
    "apply_cmr = True\n",
    "freq_min_hp = 300\n",
    "freq_max_hp = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_filter:\n",
    "    recording_processed = st.preprocessing.bandpass_filter(recording, freq_min=freq_min_hp, freq_max=freq_max_hp)\n",
    "else:\n",
    "    recording_processed = recording\n",
    "\n",
    "if apply_cmr:\n",
    "    recording_processed = st.preprocessing.common_reference(recording_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stub_test:\n",
    "    recording_processed = se.SubRecordingExtractor(recording_processed, end_frame=int(nsec_stub*recording_processed.get_sampling_frequency()))\n",
    "    recording_lfp = se.SubRecordingExtractor(recording_lfp, end_frame=int(nsec_stub*recording_lfp.get_sampling_frequency()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ts_ap = sw.plot_timeseries(recording_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Run spike sorters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.installed_sorters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorter_list = [\n",
    "    \"klusta\", # ironclust requires channel locations\n",
    "    \"tridesclous\"\n",
    "    # \"waveclus\" # waveclust errors out, \"File type '' isn't supportedERROR: MATLAB error Exit Status: 0x00000001\"\n",
    "]\n",
    "\n",
    "# this can also be done by setting global env variables: IRONCLUST_PATH, WAVECLUS_PATH\n",
    "ss.IronClustSorter.set_ironclust_path(\"D:/GitHub/ironclust\")\n",
    "ss.WaveClusSorter.set_waveclus_path(\"D:/GitHub/wave_clus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect sorter-specific parameters and defaults\n",
    "for sorter in sorter_list:\n",
    "    print(f\"{sorter} params description:\")\n",
    "    pprint(ss.get_params_description(sorter))\n",
    "    print(\"Default params:\")\n",
    "    pprint(ss.get_default_params(sorter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-specific parameters\n",
    "sorter_params = dict(\n",
    "    klusta=dict(),\n",
    "    tridesclous=dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important** To spike sort by grup, set the `grouping_property='group`. This way, the different tetrodes will be sorted separately and re-assembled after spike sorting. \n",
    "\n",
    "The `run_sorters` function does not support the `grouping_property` argument, so you need to launch spike sorters in a different way if you have groups. This next cell will create the same output dictionary as the CED pipeline.\n",
    "\n",
    "Note that the `parallel` argument allows one to run different groups (tetrodes) in parallel. This is different than the intrinsic parallelization of the spike sorters (which can be controlled by their params)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_outputs = dict()\n",
    "working_folder = spikeinterface_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for sorter_name in sorter_list:\n",
    "    print(f\"Running {sorter_name}\")\n",
    "    sorting = ss.run_sorter(sorter_name, recording_processed, output_folder=working_folder / sorter_name,\n",
    "                            grouping_property='group', parallel=True, n_jobs=4, verbose=True, \n",
    "                            raise_error=False, **sorter_params[sorter_name])\n",
    "    sorting_outputs[('rec0', sorter_name)] = sorting\n",
    "    print(f\"{sorter_name} found {len(sorting.get_unit_ids())} units!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Post-processing: extract waveforms, templates, quality metrics, extracellular features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set postprocessing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing params\n",
    "postprocessing_params = st.postprocessing.get_common_params()\n",
    "pprint(postprocessing_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note for Windows**: on Windows, we currently have some problems with the `memmap` argument. While we fix it, we recommend to set it to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) change parameters\n",
    "postprocessing_params['max_spikes_per_unit'] = 1000  # with None, all waveforms are extracted\n",
    "\n",
    "# by setting 'grouping_property' to True, everything is computed tetrode-wise (handy for manual curation)\n",
    "postprocessing_params['grouping_property'] = \"group\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set quality metric list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality metrics\n",
    "qc_list = st.validation.get_quality_metrics_list()\n",
    "print(f\"Available quality metrics: {qc_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) define subset of qc\n",
    "qc_list = [\"snr\", \"isi_violation\", \"firing_rate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set extracellular features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracellular features\n",
    "ec_list = st.postprocessing.get_template_features_list()\n",
    "print(f\"Available EC features: {ec_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) define subset of ec\n",
    "ec_list = [\"peak_to_valley\", \"halfwidth\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocess all sorting outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result_name, sorting in sorting_outputs.items():\n",
    "    rec_name, sorter = result_name\n",
    "    print(f\"Postprocessing recording {rec_name} sorted with {sorter}\")\n",
    "    tmp_folder = spikeinterface_folder / 'tmp' / sorter\n",
    "    tmp_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # set local tmp folder\n",
    "    sorting.set_tmp_folder(tmp_folder)\n",
    "    \n",
    "    # compute waveforms\n",
    "    waveforms = st.postprocessing.get_unit_waveforms(recording_processed, sorting, **postprocessing_params)\n",
    "    \n",
    "    # compute templates\n",
    "    templates = st.postprocessing.get_unit_templates(recording_processed, sorting, **postprocessing_params)\n",
    "    \n",
    "    # comput EC features\n",
    "    ec = st.postprocessing.compute_unit_template_features(recording_processed, sorting,\n",
    "                                                          feature_names=ec_list, as_dataframe=True)\n",
    "    # compute QCs\n",
    "    qc = st.validation.compute_quality_metrics(sorting, recording=recording_processed, \n",
    "                                               metric_names=qc_list, as_dataframe=True)\n",
    "    \n",
    "    # export to phy\n",
    "    phy_folder = spikeinterface_folder / 'phy' / sorter\n",
    "    phy_folder.mkdir(parents=True, exist_ok=True)\n",
    "    st.postprocessing.export_to_phy(recording_processed, sorting, phy_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Ensemble spike sorting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sorting_outputs) > 1:\n",
    "    # retrieve sortings and sorter names\n",
    "    sorting_list = []\n",
    "    sorter_names_comp = []\n",
    "    for result_name, sorting in sorting_outputs.items():\n",
    "        rec_name, sorter = result_name\n",
    "        sorting_list.append(sorting)\n",
    "        sorter_names_comp.append(sorter)\n",
    "        \n",
    "    # run multisorting comparison\n",
    "    mcmp = sc.compare_multiple_sorters(sorting_list=sorting_list, name_list=sorter_names_comp)\n",
    "    \n",
    "    # plot agreement results\n",
    "    w_agr = sw.plot_multicomp_agreement(mcmp)\n",
    "    \n",
    "    # extract ensamble sorting\n",
    "    sorting_ensemble = mcmp.get_agreement_sorting(minimum_agreement_count=2)\n",
    "    \n",
    "    print(f\"Ensemble sorting among {sorter_list} found: {len(sorting_ensemble.get_unit_ids())} units\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) save ensemble output for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_si_object(\n",
    "    \"sorting_ensemble\", sorting_ensemble, spikeinterface_folder,\n",
    "    cache_raw=False, include_properties=True, include_features=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Automatic curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define curators and thresholds\n",
    "isi_violation_threshold = 0.5\n",
    "snr_threshold = 5\n",
    "firing_rate_threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_auto_curated = []\n",
    "sorter_names_curation = []\n",
    "for result_name, sorting in sorting_outputs.items():\n",
    "    rec_name, sorter = result_name\n",
    "    sorter_names_curation.append(sorter)\n",
    "    \n",
    "    num_frames = recording_processed.get_num_frames()\n",
    "    # firing rate threshold\n",
    "    sorting_curated = st.curation.threshold_firing_rates(\n",
    "        sorting,\n",
    "        duration_in_frames=num_frames,\n",
    "        threshold=firing_rate_threshold, \n",
    "        threshold_sign='less'\n",
    "    )\n",
    "    \n",
    "    # isi violation threshold\n",
    "    sorting_curated = st.curation.threshold_isi_violations(\n",
    "        sorting,\n",
    "        duration_in_frames=num_frames,\n",
    "        threshold=isi_violation_threshold, \n",
    "        threshold_sign='greater'\n",
    "    )\n",
    "    \n",
    "    # isi violation threshold\n",
    "    sorting_curated = st.curation.threshold_snrs(\n",
    "        sorting,\n",
    "        recording=recording_processed,\n",
    "        threshold=snr_threshold, \n",
    "        threshold_sign='less'\n",
    "    )\n",
    "    sorting_auto_curated.append(sorting_curated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Quick save to NWB; writes only the spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To complete the full conversion for other types of data, use the external script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name your NWBFile and decide where you want it saved\n",
    "nwbfile_path = base_path / \"Syntalos.nwb\"\n",
    "\n",
    "# Enter Session and Subject information here\n",
    "session_description = \"Enter session description here.\"\n",
    "\n",
    "# Choose the sorting extractor from the notebook environment you would like to write to NWB\n",
    "chosen_sorting_extractor = sorting_outputs[('rec0', 'ironclust')]\n",
    "\n",
    "quick_write(\n",
    "    intan_folder_path=syntalos_folder,\n",
    "    session_description=session_description,\n",
    "    save_path=nwbfile_path,\n",
    "    sorting=chosen_sorting_extractor,\n",
    "    lfp=recording_lfp,\n",
    "    timestamps=recording.get_timestamps(),\n",
    "    overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
